{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Concepts\n",
    "\n",
    "## The Agent-Environment Interface\n",
    "The agent-enviroment interface can be summarized as a sequence $\\cdots S_tA_tR_{t+1}S_{t+1}A_{t+1}\\cdots$.\n",
    "\n",
    "The tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ can be described as:\n",
    "1. At time $t$, the agent observes environment's state $S_t$ and takes action $A_t$;\n",
    "2. This action drives the environment to change its state to $S_{t+1}$, and the agent receives reward $R_{t+1}$ from the enviroment;\n",
    "3. The Agent takes action $A_{t+1}$ and the cycle continues.\n",
    "\n",
    "The agent's goal is to maximize its total reward in this process. \n",
    "- If the process is finite, we say the agent is doing episodic tasks, and the total reward is defined as $R_1 + \\ldots + R_T$.\n",
    "- If the process is infinite, we say the agent is doing continous task, and the total reward is defined as $R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\ldots$, where $ 0 < \\gamma < 1 $ is a discount factor that ensures the sum is finite.\n",
    "\n",
    "For convenience, we use the unified notation $\\sum_{k}\\gamma^{k} R_{k+1}$, where $ 0 < \\gamma \\leq 1$, $\\gamma = 1$ iff for episodic tasks.\n",
    "\n",
    "Often, we are interesed in the return $G_t$, which is defined as the total reward after time $t$. This can be viewed as the feedback to the agent's action $A_t$ taken on state $S_t$. By definition, $G_t = R_{t+1} + \\gamma R_{t+2} + \\ldots$. Note the recurrsion relation $G_t = R_{t+1} + \\gamma G_{t+1}$.\n",
    "\n",
    "## Markov Property\n",
    "This says $S_{t+1}$ and $R_{t+1}$ only depends on $S_t, A_t$, but not on any earlier events.\n",
    "Thus we may write $P(S_{t+1}, R_{t+1} | S_t, A_t )$. Note we can further factorize this as $P(S_{t+1}, R_{t+1} | S_t, A_t ) = P(S_{t+1} | S_t, A_t) P(R_{t+1} | S_{t}, A_{t}, S_{t+1}).$ Note that $R_{t+1}$ has dependency on three variables. \n",
    "\n",
    "We can simplify the notation by writing $s, a, s'$ to denote $S_t, A_t, S_{t+1}$. Two more notations:\n",
    "- $p(s' | s, a)$ denotes the transition probability $(s, a) \\rightarrow s' $\n",
    "- $r(s, a, s')$ denotes the expectation $\\mathbb{E}(R_{t+1} | S_{t} = s, A_{t} = a, S_{t+1} = s')$.\n",
    "\n",
    "## Value Functions, Bellman Equation\n",
    "We cannot talk about values without the policy. A policy maps the state to action. This can be deterministic or probabilistic. Usually, we denote the policy by $\\pi(s)$, sometimes it's a distribution $p(a|s)$. For deterministic policy, it's just a value.\n",
    "\n",
    "We can define value for a state or a state-action pair. Informally, the value of a state is the expected return when starting from $s$ and following $\\pi$ thereafter. The value of a state-pair is the expected return when starting from $s$ and take action $a$.\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi} (G_t | S_t = s )$$\n",
    "$$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}(G_t | S_t = s, A_t = a)$$\n",
    "The notation is a bit sloppy here: $t$ appears on the RHS but not on the LHS. So the LHS is really $v_{\\pi}(S_t = s)$ and $q_{\\pi}(S_t = s, A_t = a)$.\n",
    "\n",
    "\\begin{align}\n",
    "q(s, a) & = \\mathbb{E}[ G_t | s, a ] \\\\\n",
    "        & = \\mathbb{E}[ R_{t+1} + \\gamma G_{t+1} | s, a ] \\\\\n",
    "        & = \\sum_{s'} p(s' | s, a) \\mathbb{E} [R_{t+1} + \\gamma G_{t+1} | s, a, s'] \\\\\n",
    "        & = \\sum_{s'} p(s' | s, a) (r(s, a, s') + \\gamma v(s')) \\\\\n",
    "v(s)    & = \\sum_a p(a | s) q(s, a) \\\\\n",
    "        & = \\sum_a p(a | s) \\sum_{s'} p(s' | s, a) (r(s, a, s') + \\gamma v(s'))\n",
    "\\end{align}\n",
    "The last equation that reveals the recurssion relation of $v(s)$ and $v(s')$ is known as the Bellman equation.\n",
    "\n",
    "The Bellman equation has two implications:\n",
    "1. It expresses $v(s)$ as a solution of a linear system.\n",
    "2. It expresses $v(s)$ as a fixed point of a mapping. This suggests we can use iterative methods to solve for $v$. This gives the basic form of policy evaluation.\n",
    "\n",
    "## Optimal Value Functions\n",
    "There exists an optimal policy $\\pi_{*}$ such that for any other policy $\\pi$, we have $v_{\\pi_{*}}(s) \\geq v_{\\pi}(s), \\forall s \\in S$.\n",
    "\n",
    "\\begin{align}\n",
    "v_*(s)     & = \\max_{\\pi} v_{\\pi}(s) \\\\\n",
    "q_*(s, a)  & = \\max_{\\pi} q_{\\pi}(s, a)\n",
    "\\end{align}\n",
    "\n",
    "$v_*(s)$ satisfies the Bellman optimal equation. The derivation of the Bellman optimal equation is similar to the regular Bellman equation. The main difference is: $v_*(s) = \\max_a q(s, a)$ instead of averaging over the distribution $p(a|s)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "Policy iteration alternates between policy evaluation and policy improvement:\n",
    "- Start with an arbitrary policy $\\pi$.\n",
    "- In policy evaluation, we use an iterative method based on the Bellman equation to find $v(s)$. This gives us $q(s, a)$ too.\n",
    "- In policy improvment, we set $\\pi(s) = \\text{argmax}_a q(s, a)$.\n",
    "\n",
    "## Value Iteration\n",
    "Valute iteration is an approximation of policy iteration. In policy evaluation, we iterate until converge. In value iteration, we only iterate once. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

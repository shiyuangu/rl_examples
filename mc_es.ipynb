{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sutton's book Chapter 5: Blackjack with Monte Carlo Exploring Start\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DECK_OF_CARDS = [1,2,3,4,5,6,7,8,9,10,10,10,10] * 4 #J, Q, K count as 10.\n",
    "DEALDER_STICK_THRESHOLD = 17 # same as Sutton's and Vega's rule: https://www.youtube.com/watch?v=SWdPf21v5Ak\n",
    "\n",
    "def sum_hand(hand):\n",
    "    min_s = sum(hand)\n",
    "    max_s = sum(hand)\n",
    "    if 1 in hand:\n",
    "        max_s = min_s  + 10\n",
    "    if max_s <= 21 and max_s > min_s:\n",
    "        ## Has a usable ACE.\n",
    "        return (max_s, 1)\n",
    "    else:\n",
    "        return (min_s, 0)\n",
    "    \n",
    "def busted(hand):\n",
    "    (s, _) = sum_hand(hand)\n",
    "    return s > 21    \n",
    "    \n",
    "def convert_cards_to_state(my_cards, dealer_card):\n",
    "    (s, usable_ace) = sum_hand(my_cards)\n",
    "    # state is 0-based, for easy indexing with ndarray.\n",
    "    return (s-1, dealer_card-1, usable_ace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePlayer(object):\n",
    "    # Return 1 if hit, 0 if stick.\n",
    "    def action(self, cards, rival_card):\n",
    "        raise NotImplementedError(\"action() is not implemented.\")\n",
    "\n",
    "class Dealer(BasePlayer):\n",
    "    def __init__(self):\n",
    "        self.stick_threshold = DEALDER_STICK_THRESHOLD\n",
    "    def action(self, cards, rival_card):\n",
    "        (s, _) = sum_hand(cards) #s>=1 \n",
    "        return int(self.stick_threshold > s) #stick for 17 or greater, same as Sutton's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self):\n",
    "        # stats is a 5D tensor. \n",
    "        # The first 3 dimensions describe state. The 4th dimension describes\n",
    "        # action (0 for stick, 1 for hit). The 5th dimension describes the\n",
    "        # reward: {-1, 0, 1}.\n",
    "        # self.states[i][j][k][a][r] saves counts of how many times we have\n",
    "        # reward `r` when we take action `a` for state s = (i, j, k).\n",
    "        self.stats = np.zeros((21, 10, 2, 2, 3), dtype='int64') # Init to 1 instead of 0 to smooth out.\n",
    "        # q is a 4D tensor that represents q(s, a).\n",
    "        self.q = np.zeros((21, 10, 2, 2), dtype='float64')\n",
    "        # pi is a 4D tensor that represents pi(a|s), last dimension is the action\n",
    "        self.pi = np.zeros((21, 10, 2, 2), dtype='float64')\n",
    "        #pi[:,:,:,1] is the probability of hit while pi[:,:,:,0] is the probability of stick\n",
    "        self.pi[0:11,:,:,:] = [0., 1.] #Always hit if the cards sum to [1,11]; note that this part of pi should never get updated\n",
    "        self.pi[11:19,:,:,:]=[0.,1.] #same as Sutton's initial policy, hit if [12,19]; stick if 20,21\n",
    "        self.pi[19,:,:,:] = [1., 0.] \n",
    "        \n",
    "    # state_action_seq is a list of tuple. \n",
    "    def update(self, state_action_seq, final_reward):\n",
    "        #print 'updating with state_action_seq:',state_action_seq, ' with reward: ', final_reward\n",
    "        #update q(s,a) using the episode(ie., state_action_seq). \n",
    "        for state_action in state_action_seq:  \n",
    "            (i,j,k,a) = state_action #i=sum(play_cards), 0<=i<=20, 0<=j<=9, k=0 or 1,  \n",
    "            if i <= 10:\n",
    "                # No update: we always hit.\n",
    "                continue\n",
    "            if i == 20:\n",
    "                # No update: we always stick.\n",
    "                continue\n",
    "            # reward is in {-1, 0, 1}    \n",
    "            self.stats[i,j,k,a,final_reward + 1] += 1\n",
    "            # q stores average reward so far.\n",
    "            # Denominator is the # of samples; the numerator is the net reward accumulated from +1 and -1 cases.\n",
    "            self.q[i,j,k,a] = (self.stats[i,j,k,a,2] - self.stats[i,j,k,a,0]) * 1.0 / np.sum(self.stats[i,j,k,a,:])\n",
    "            # If q[i,i,k,0] > q[i,j,k,1], argsort returns [1,0].\n",
    "            # This is exactly how greedy algorithm updates policy.\n",
    "        #greedy update pi(a|s) based on estimated q(s,a)\n",
    "        self.pi[i,j,k,:] = np.argsort(self.q[i,j,k,:]) \n",
    "  \n",
    "    def action(self, state):#greedy \n",
    "        return np.argmax(self.pi[*state])\n",
    "    \n",
    "    # Visualize matrix as a grayscale image, assuming entries are in [0,1].\n",
    "    def imshow(self, ax, matrix, title):\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(matrix, cmap='gray',vmin=0., vmax=1.)        \n",
    "\n",
    "        ax.set_xlabel('Dealer Card')\n",
    "        ax.set_xticks(np.arange(10))\n",
    "        ax.set_xticklabels(np.arange(1,11,1).astype('S2'))\n",
    "        \n",
    "        ax.set_ylabel('Player Card')\n",
    "        ax.set_yticks(np.arange(9))\n",
    "        ax.set_yticklabels(np.arange(20,11,-1).astype('S2'))\n",
    "        \n",
    "        ax.spines['right'].set_color('none')\n",
    "        ax.spines['top'].set_color('none')      \n",
    "    \n",
    "    # Visualize the hard decision matrix and the soft (Bayesian) decision matrix.\n",
    "    def visualize(self):\n",
    "        fig = plt.figure(figsize=(12, 9), dpi=80)\n",
    "        \n",
    "        usable_ace = 0\n",
    "        ax = plt.subplot(221)\n",
    "        decision = (self.pi[11:20,:,usable_ace,1] > self.pi[11:20,:,usable_ace,0]).astype(int)\n",
    "        self.imshow(ax, np.flipud(decision), 'Decision (No usable Ace)')\n",
    "        \n",
    "        ax = plt.subplot(222)\n",
    "        bayesian_decision = self.bayesian_decision(0)\n",
    "        self.imshow(ax, bayesian_decision, 'Bayesian Decision (No usable Ace)')\n",
    "        \n",
    "        usable_ace = 1\n",
    "        ax = plt.subplot(223)\n",
    "        decision = (self.pi[11:,:,usable_ace,1] > self.pi[11:,:,usable_ace,0]).astype(int)\n",
    "        self.imshow(ax, np.flipud(decision), 'Decision (with usable Ace)')\n",
    "\n",
    "        ax = plt.subplot(224)\n",
    "        bayesian_decision = self.bayesian_decision(usable_ace)\n",
    "        self.imshow(ax, bayesian_decision, 'Bayesian Decision (with usable Ace)')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPlayer(BasePlayer):\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        self.state_action_pairs = []\n",
    "    def action(self, cards, rival_card):\n",
    "        s = convert_cards_to_state(cards, rival_card) #s is 0-based \n",
    "        a = self.policy.action(s)\n",
    "        (i,j,k) = s  # \n",
    "        self.state_action_pairs.append((i,j,k,a))\n",
    "        return a\n",
    "    def update_policy(self, reward): #this combines the policy evaulation and improvement step;  \n",
    "        self.policy.update(self.state_action_pairs, reward)\n",
    "    def reset_states(self):\n",
    "        self.state_action_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple debugger (to avoid clutter in main code.)\n",
    "class Dbg:\n",
    "    # level can be 0 or 1. (0 means silent.)\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "    def print_hands(self, game):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'dealer cards: ', game.dealer_cards \n",
    "        print 'player cards: ', game.player_cards\n",
    "    def on_dealer_action(self, action):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'dealer action: ', action\n",
    "    def on_player_action(self, action):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'player action: ', action\n",
    "    def print_bust_status(self, dealer, player):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        if player and dealer:\n",
    "            print 'both busted'\n",
    "        elif player:\n",
    "            print 'player busted'\n",
    "        elif dealer:\n",
    "            print 'dealer busted'\n",
    "        else:\n",
    "            pass\n",
    "    def print_sum_of_hands(self, dealer_sum, player_sum):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'dealer sum: {}, player sum: {}'.format(dealer_sum, player_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    \"\"\"\n",
    "    Shoe game as in https://www.youtube.com/watch?v=SWdPf21v5Ak\n",
    "    \"\"\"\n",
    "    def __init__(self, dealer, player, debug_level):\n",
    "        self.dealer = dealer\n",
    "        self.player = player\n",
    "        self.dealer_cards = []\n",
    "        self.player_cards = []\n",
    "        self.cards = DECK_OF_CARDS[:]\n",
    "        #random.shuffle(self.cards) #Sutton assumes infinite cards. Use sample with replacement  instead. \n",
    "        self.card_index = 0\n",
    "        self.dbg = Dbg(debug_level)\n",
    "    def reset(self):\n",
    "        self.player.reset_states()\n",
    "        self.dealer_cards = []\n",
    "        self.player_cards = []\n",
    "\n",
    "    def on_player_action(self, action):\n",
    "        if action == 1:\n",
    "           #Sutton assumes infinite cards.\n",
    "            self.player_cards.extend(np.random.choice(self.cards,size=1))\n",
    "        self.dbg.on_player_action(action)\n",
    "    def on_dealer_action(self, action):\n",
    "        if action == 1:\n",
    "            self.dealer_cards.extend(np.random.choice(self.cards,size=1))\n",
    "        self.dbg.on_dealer_action(action)\n",
    "    \n",
    "    def check_bust(self):\n",
    "        dealer_busted = busted(self.dealer_cards)\n",
    "        player_busted = busted(self.player_cards)\n",
    "        self.dbg.print_bust_status(dealer_busted, player_busted)\n",
    "        return (dealer_busted, player_busted)\n",
    "    \n",
    "    # Returns the reward for the player. Can be one of {-1, 0, 1}.\n",
    "    def compare_hands(self):\n",
    "        d_sum, _ = sum_hand(self.dealer_cards)\n",
    "        p_sum, _ = sum_hand(self.player_cards)        \n",
    "        return np.sign(p_sum - d_sum)\n",
    "        \n",
    "    # Returns 1 iff player wins.    \n",
    "    def play(self,dealer_init_cards, player_init_cards, a):\n",
    "        \"\"\"\n",
    "        each play generates an episode starting from state the initial states and action \n",
    "        dealer_init_cards, player_init_cards: List[Int] of size 2\n",
    "        a: player first action,  1 for hit ; 0 for stick, \n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick sanity check of the game driver.\n",
    "dealer = Dealer()\n",
    "policy = Policy()\n",
    "player = PolicyPlayer(policy)\n",
    "game = Game(dealer, player, 1)\n",
    "r = game.play()\n",
    "print 'result: ', r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self):\n",
    "        self.player = PolicyPlayer(Policy())\n",
    "    def play(self, num_of_training, debug_level):\n",
    "        dealer = Dealer()\n",
    "        mini_batch = 5 # We shuffle cards after 5 rounds of play.\n",
    "        progress_bar_iters = 100 # This is the number of times we \"report progress\", e.g. refreshing dynamic plots.\n",
    "        progress_bar_step_size = (num_of_training / mini_batch) / progress_bar_iters\n",
    "        old_decision = np.zeros((9,10))\n",
    "        old_q = np.zeros((9,10))\n",
    "        epoch = []\n",
    "        delta_q = []\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        j = 0\n",
    "        for i in xrange(num_of_training / mini_batch):\n",
    "            if i > 0 and progress_bar_iters > 0 and i % progress_bar_step_size == 0:\n",
    "                plt.clf()\n",
    "                if i < num_of_training / mini_batch - 1:\n",
    "                    display.clear_output(wait=True)                \n",
    "                epoch.append(j)\n",
    "                j += 1\n",
    "                new_q = self.player.policy.q[11:20,:,0,0]\n",
    "                delta_q.append(np.max(np.abs(new_q - old_q)))\n",
    "                old_q = np.copy(new_q)\n",
    "                \n",
    "                plt.subplot(211)\n",
    "                plt.plot(epoch, delta_q, 'b-')\n",
    "                plt.xlim(0, progress_bar_iters)\n",
    "                plt.ylim(0, 1.1)\n",
    "                \n",
    "                plt.subplot(223)\n",
    "                decision = (self.player.policy.pi[11:20,:,0,1] > self.player.policy.pi[11:20,:,0,0]).astype(int)\n",
    "                plt.imshow(np.flipud(decision), cmap='gray', vmin=0., vmax=1.)\n",
    "                plt.xlabel('Dealer Card')\n",
    "                plt.xticks(np.arange(10), np.arange(1,11,1).astype('S2'))\n",
    "                plt.ylabel('My Card')\n",
    "                plt.yticks(np.arange(9), np.arange(20,11,-1).astype('S2'))\n",
    "                ax = plt.gca()\n",
    "                ax.spines['right'].set_color('none')\n",
    "                ax.spines['top'].set_color('none')        \n",
    "                plt.colorbar()                \n",
    "                 \n",
    "                display.display(plt.gcf())\n",
    "  \n",
    "            if debug_level > 0:\n",
    "                print '\\n\\n === Game {} === \\n\\n'.format(i+1)\n",
    "            game = Game(dealer, self.player, debug_level)\n",
    "            \n",
    "            # Play `mini_batch` number of games without shuffling.\n",
    "            for i in xrange(mini_batch): #sgu: no shuffle? also the Sutton's example assume infinit cards(sample w/ replacement)\n",
    "                r = game.play() \n",
    "                self.player.update_policy(r)\n",
    "                self.player.reset_states()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner = Learner()\n",
    "learner.play(5000000, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.player.policy.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "i,j,k = 16,6,1\n",
    "policy = learner.player.policy\n",
    "print policy.stats[i,j,k,:,:]\n",
    "print policy.q[i,j,k,:]\n",
    "print policy.pi[i,j,k]\n",
    "policy.plot_beta((i,j,k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

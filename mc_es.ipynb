{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sutton's book Chapter 5: Blackjack with Monte Carlo Exploring Start\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DECK_OF_CARDS = [1,2,3,4,5,6,7,8,9,10,10,10,10] * 4 #J, Q, K count as 10.\n",
    "DEALDER_STICK_THRESHOLD = 17 # same as Sutton's and Vega's rule: https://www.youtube.com/watch?v=SWdPf21v5Ak\n",
    "\n",
    "def sum_hand(hand):\n",
    "    min_s = sum(hand)\n",
    "    max_s = sum(hand)\n",
    "    if 1 in hand:\n",
    "        max_s = min_s  + 10\n",
    "    if max_s <= 21 and max_s > min_s:\n",
    "        ## Has a usable ACE.\n",
    "        return (max_s, 1)\n",
    "    else:\n",
    "        return (min_s, 0)\n",
    "    \n",
    "def busted(hand):\n",
    "    (s, _) = sum_hand(hand)\n",
    "    return s > 21    \n",
    "    \n",
    "def convert_cards_to_state(my_cards, dealer_card):\n",
    "    (s, usable_ace) = sum_hand(my_cards)\n",
    "    # state is 0-based, for easy indexing with ndarray.\n",
    "    return (s-1, dealer_card-1, usable_ace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePlayer(object):\n",
    "    # Return 1 if hit, 0 if stick.\n",
    "    def action(self, cards, rival_card):\n",
    "        raise NotImplementedError(\"action() is not implemented.\")\n",
    "\n",
    "class Dealer(BasePlayer):\n",
    "    def __init__(self):\n",
    "        self.stick_threshold = DEALDER_STICK_THRESHOLD\n",
    "    def action(self, cards):\n",
    "        (s, _) = sum_hand(cards) #s>=1 \n",
    "        return int(self.stick_threshold > s) #stick for 17 or greater, same as Sutton's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self):\n",
    "        # stats is a 5D tensor. \n",
    "        # The first 3 dimensions describe state. The 4th dimension describes\n",
    "        # action (0 for stick, 1 for hit). The 5th dimension describes the\n",
    "        # reward: {-1, 0, 1}.\n",
    "        # self.states[i][j][k][a][r] saves counts of how many times we have\n",
    "        # reward `r` when we take action `a` for state s = (i, j, k) (s is 0-based).\n",
    "        self.stats = np.zeros((21, 10, 2, 2, 3), dtype='int64') # Init to 1 instead of 0 to smooth out.\n",
    "        # q is a 4D tensor that represents q(s, a).\n",
    "        self.q = np.zeros((21, 10, 2, 2), dtype='float64')\n",
    "        # pi is a 4D tensor that represents pi(a|s), last dimension is the action\n",
    "        self.pi = np.zeros((21, 10, 2, 2), dtype='float64')\n",
    "        #pi[:,:,:,1] is the probability of hit while pi[:,:,:,0] is the probability of stick\n",
    "        self.pi[0:11,:,:,:] = [0., 1.] #Always hit if the cards sum to [1,11]; note that this part of pi should never get updated\n",
    "        self.pi[11:19,:,:,:]=[0.,1.] #same as Sutton's initial policy, hit if [12,19]; stick if 20,21\n",
    "        self.pi[19,:,:,:] = [1., 0.] \n",
    "        \n",
    "    def update(self, state_action_seq, final_reward):\n",
    "        #update q(s,a) snd pi(a|s) using the episode(ie., state_action_seq) and the reward \n",
    "        #For blackjack, the states in the state_action_seq are all different; first visit MC and every visit MC are the same\n",
    "        for state_action in state_action_seq:  \n",
    "            (i,j,k,a) = state_action #i=sum(play_cards), 0<=i<=20, 0<=j<=9, k=0 or 1,  \n",
    "            # reward is in {-1, 0, 1}    \n",
    "            self.stats[i,j,k,a,final_reward + 1] += 1\n",
    "            # q stores average reward so far.\n",
    "            # Denominator is the # of samples; the numerator is the net reward accumulated from +1 and -1 cases.\n",
    "            self.q[i,j,k,a] = (self.stats[i,j,k,a,2] - self.stats[i,j,k,a,0]) * 1.0 / np.sum(self.stats[i,j,k,a,:])\n",
    "                        \n",
    "            #greedy update pi(a|s) based on estimated q(s,a)\n",
    "            if i <= 10:\n",
    "                # No update: we always hit.\n",
    "                continue\n",
    "            if i == 20:\n",
    "                # No update: we always stick.\n",
    "                continue                     \n",
    "            # If q[i,i,k,0] > q[i,j,k,1], argsort returns [1,0].            \n",
    "            self.pi[i,j,k,:] = np.argsort(self.q[i,j,k,:]) \n",
    "  \n",
    "    def action(self, state):#greedy \n",
    "        return np.argmax(self.pi[state])\n",
    "    \n",
    "    # Visualize matrix as a grayscale image, assuming entries are in [0,1].\n",
    "    def imshow(self, ax, matrix, title):\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(matrix, cmap='gray',vmin=0., vmax=1.)        \n",
    "\n",
    "        ax.set_xlabel('Dealer Card')\n",
    "        ax.set_xticks(np.arange(10))\n",
    "        ax.set_xticklabels(np.arange(1,11,1).astype('S2'))\n",
    "        \n",
    "        ax.set_ylabel('Player Card')\n",
    "        ax.set_yticks(np.arange(9))\n",
    "        ax.set_yticklabels(np.arange(20,11,-1).astype('S2'))\n",
    "        \n",
    "        ax.spines['right'].set_color('none')\n",
    "        ax.spines['top'].set_color('none')      \n",
    "    \n",
    "    # Visualize the hard decision matrix and the soft (Bayesian) decision matrix.\n",
    "    def visualize(self):\n",
    "        fig = plt.figure(figsize=(12, 9), dpi=80)\n",
    "        \n",
    "        usable_ace = 0\n",
    "        ax = plt.subplot(221)\n",
    "        decision = (self.pi[11:20,:,usable_ace,1] > self.pi[11:20,:,usable_ace,0]).astype(int)\n",
    "        self.imshow(ax, np.flipud(decision), 'Decision (No usable Ace)')\n",
    "        \n",
    "        usable_ace = 1\n",
    "        ax = plt.subplot(223)\n",
    "        decision = (self.pi[11:,:,usable_ace,1] > self.pi[11:,:,usable_ace,0]).astype(int)\n",
    "        self.imshow(ax, np.flipud(decision), 'Decision (with usable Ace)')\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPlayer(BasePlayer):\n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        self.state_action_pairs = []\n",
    "        \n",
    "    def action(self, cards, rival_card,a=None):\n",
    "        s = convert_cards_to_state(cards, rival_card) #s is 0-based \n",
    "        if a is None: \n",
    "            a = self.policy.action(s)\n",
    "        (i,j,k) = s  # \n",
    "        self.state_action_pairs.append((i,j,k,a))\n",
    "        return a\n",
    "    \n",
    "    #def update_policy(self, reward): #this combines the policy evaulation and improvement step;  \n",
    "       # self.policy.update(self.state_action_pairs, reward)\n",
    "    def reset_states(self):\n",
    "        self.state_action_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple debugger (to avoid clutter in main code.)\n",
    "class Dbg(object):\n",
    "    # level can be 0 or 1. (0 means silent.)\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "    def print_hands(self, game):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'dealer cards: ', game.dealer_cards \n",
    "        print 'player cards: ', game.player_cards\n",
    "        \n",
    "    def print_hand(self,cards):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'cards: ', cards \n",
    "        \n",
    "    def on_dealer_action(self, action):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'dealer action: ', action\n",
    "    def on_player_action(self, action):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'player action: ', action\n",
    "    def print_bust_status(self, dealer, player):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        if player and dealer:\n",
    "            print 'both busted'\n",
    "        elif player:\n",
    "            print 'player busted'\n",
    "        elif dealer:\n",
    "            print 'dealer busted'\n",
    "        else:\n",
    "            pass\n",
    "    def print_sum_of_hands(self, dealer_sum, player_sum):\n",
    "        if self.level == 0:\n",
    "            return\n",
    "        print 'dealer sum: {}, player sum: {}'.format(dealer_sum, player_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    \"\"\"\n",
    "    Blackjack shoe game as in https://www.youtube.com/watch?v=SWdPf21v5Ak\n",
    "    \"\"\"\n",
    "    def __init__(self, dealer, player, debug_level=0):\n",
    "        self.dealer = dealer\n",
    "        self.player = player\n",
    "        self.cards = DECK_OF_CARDS[:]\n",
    "        #random.shuffle(self.cards) #Sutton assumes infinite cards. Use sample with replacement  instead. \n",
    "        self.dbg = Dbg(debug_level)\n",
    "        \n",
    "    def on_player_action(self, action):\n",
    "        if action == 1:\n",
    "           #Sutton assumes infinite cards.\n",
    "            self.player_cards.extend(np.random.choice(self.cards,size=1))\n",
    "        self.dbg.on_player_action(action)\n",
    "    def on_dealer_action(self, action):\n",
    "        if action == 1:\n",
    "            self.dealer_cards.extend(np.random.choice(self.cards,size=1))\n",
    "        self.dbg.on_dealer_action(action)\n",
    "        \n",
    "    # Returns 1 iff player wins.    \n",
    "    def play(self,dealer_init_cards, player_init_cards, a):\n",
    "        \"\"\"\n",
    "        each play generates an episode starting from state the initial states and action \n",
    "        dealer_init_cards, player_init_cards: List[Int] of size 2; must be non-busted state; \n",
    "        a: player first action,  1 for hit ; 0 for stick, \n",
    "        return: 1 if the player wins, -1 if the player loses, 0 if draw\n",
    "        \"\"\" \n",
    "        self.player.reset_states() #reset the player's memory of state_action pair\n",
    "        self.dealer_cards = list(dealer_init_cards)\n",
    "        self.player_cards = list(player_init_cards)\n",
    "        \n",
    "        assert not busted(dealer_init_cards) and not busted(player_init_cards), \"Error: Invalid initial state!\"\n",
    "        \n",
    "        #player's turn\n",
    "        self.player.action(self.player_cards,self.dealer_cards[0],a)\n",
    "        while a:\n",
    "            self.on_player_action(a)  # update the states \n",
    "            if busted(self.player_cards): \n",
    "                return -1 \n",
    "            a = self.player.action(self.player_cards, self.dealer_cards[0])\n",
    "    \n",
    "        self.on_player_action(a)\n",
    "        \n",
    "        #dealer's turn\n",
    "        a = self.dealer.action(self.dealer_cards)   \n",
    "        while a:\n",
    "            self.on_dealer_action(a)  # update the states \n",
    "            if busted(self.dealer_cards): \n",
    "                return 1\n",
    "            a = self.dealer.action(self.dealer_cards) #dealer sticks to the same rule regardless of the player's cards\n",
    "            \n",
    "        #both dealer and player stick \n",
    "        d_sum, _ = sum_hand(self.dealer_cards)\n",
    "        p_sum, _ = sum_hand(self.player_cards)\n",
    "        return np.sign(p_sum - d_sum)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self):\n",
    "        self.player_policy = Policy()\n",
    "        self.player = PolicyPlayer(self.player_policy)\n",
    "        self.dealer = Dealer()\n",
    "        self.game = Game(self.dealer,self.player)\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        learn the optimal policy by MC with exploring starts \n",
    "        return: optimal policy pi(a|s)\n",
    "        \"\"\"\n",
    "        np.random.seed(4)\n",
    "        max_iter = int(1e8) #reduce max_iter to 1e6 generates correct result except for the case player=12, dealer=4, use_able=0 \n",
    "        for i in range(max_iter): \n",
    "            dealer_init_cards = np.random.choice(self.game.cards,size=2,replace=True)\n",
    "            player_init_cards = np.random.choice(self.game.cards,size=2, replace=True)\n",
    "            #dealer_init_cards = np.array([10,2])\n",
    "            #player_init_cards = np.array([10,3])\n",
    "            if not busted(dealer_init_cards) and not busted(player_init_cards):\n",
    "                for a in [0,1]:\n",
    "                    rt = self.game.play(dealer_init_cards, player_init_cards,a)\n",
    "                    self.player_policy.update(self.player.state_action_pairs,rt) \n",
    "                    #print \"i=%d,a=%d,rt=%d,dealer_cards=%s,player_cards=%s, player_mem=%s\" % (i,a,rt,self.game.dealer_cards,self.game.player_cards,self.game.player.state_action_pairs)\n",
    "                    #print \"\\t self.player_policy.stats[12,9,0,%d]:%s\" % (a,self.player_policy.stats[12,9,0,a])\n",
    "        return self.player_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner = Learner()\n",
    "optimal_policy = learner.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 10, 2, 2)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.pi.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.pi[11:,:,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.pi[11:,:,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the result generated maxiter=1e8 as the true solution \n",
    "np.save(\"optimal_policy_pi.npy\",optimal_policy.pi)\n",
    "np.save(\"optimal_policy_q.npy\",optimal_policy.q)\n",
    "np.save(\"optimal_policy_stats.npy\",optimal_policy.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0, array([-0.20951887, -0.21420273]))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#player_cards=12, dealer up cards =3, useable=False is a difficult case due to the probability are very closed\n",
    "i,j,k = 11,3,0\n",
    "optimal_policy.pi[i,j,k,0], optimal_policy.pi[i,j,k,1], optimal_policy.q[i,j,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 10, 2)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.v = optimal_policy.q.max(axis=3)\n",
    "optimal_policy.v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.44785519, -0.10678826, -0.08347956, -0.05684099, -0.01460637,\n",
       "         0.01084011, -0.08865979, -0.16056134, -0.23593767, -0.33511414],\n",
       "       [-0.46287745, -0.12778811, -0.0957108 , -0.05883199, -0.02082324,\n",
       "        -0.00818187, -0.11758063, -0.18849812, -0.26870659, -0.35785099],\n",
       "       [-0.48606211, -0.13857426, -0.10718373, -0.06916031, -0.03461736,\n",
       "        -0.01275178, -0.14903936, -0.21756229, -0.29625363, -0.38011831],\n",
       "       [-0.49479843, -0.10955807, -0.07641167, -0.04021323, -0.00908444,\n",
       "         0.0271619 , -0.06487053, -0.21094444, -0.28326774, -0.3674717 ],\n",
       "       [-0.42532375, -0.0213865 ,  0.00927136,  0.03860363,  0.07064324,\n",
       "         0.11228193,  0.08413202, -0.0619964 , -0.21095434, -0.30122598],\n",
       "       [-0.33544187,  0.0770331 ,  0.10357005,  0.13002546,  0.15461072,\n",
       "         0.19632981,  0.17093386,  0.09947776, -0.05521984, -0.2130298 ],\n",
       "       [-0.21577364,  0.18200086,  0.20519892,  0.23240048,  0.25711438,\n",
       "         0.28614068,  0.25626583,  0.19695832,  0.1156482 , -0.04394371],\n",
       "       [-0.10454546,  0.24001635,  0.25902605,  0.28242119,  0.30806811,\n",
       "         0.3330706 ,  0.29114427,  0.23047095,  0.15616895,  0.05993024],\n",
       "       [-0.51915699, -0.2544021 , -0.2323214 , -0.20951887, -0.16682218,\n",
       "        -0.15526406, -0.21225376, -0.26918513, -0.33993525, -0.42058177],\n",
       "       [-0.55265681, -0.29358188, -0.251122  , -0.21024748, -0.16630909,\n",
       "        -0.15591492, -0.26827546, -0.32368723, -0.38662127, -0.46256974],\n",
       "       [-0.5861964 , -0.29199658, -0.25332703, -0.21227626, -0.1671034 ,\n",
       "        -0.15409235, -0.32126237, -0.37210519, -0.43023762, -0.50097313],\n",
       "       [-0.61391818, -0.29188022, -0.25290221, -0.21246996, -0.16917481,\n",
       "        -0.15319132, -0.36977028, -0.4177756 , -0.47117098, -0.53686865],\n",
       "       [-0.6421792 , -0.29271221, -0.25363188, -0.20995712, -0.16893275,\n",
       "        -0.15339009, -0.4136773 , -0.45801248, -0.51070702, -0.56953727],\n",
       "       [-0.63835208, -0.15182589, -0.11822211, -0.08140438, -0.04460554,\n",
       "         0.01184012, -0.10614524, -0.38239405, -0.42291448, -0.46375933],\n",
       "       [-0.37579054,  0.12160899,  0.14910093,  0.17770609,  0.19870984,\n",
       "         0.28317531,  0.39986064,  0.10510865, -0.18340588, -0.24179947],\n",
       "       [-0.1172396 ,  0.38832864,  0.40474216,  0.42229413,  0.43887412,\n",
       "         0.49608473,  0.61564354,  0.59438313,  0.28785387, -0.01929999],\n",
       "       [ 0.14536814,  0.63974507,  0.65131546,  0.66088997,  0.67134549,\n",
       "         0.70415671,  0.77362397,  0.79226064,  0.75834913,  0.43506478],\n",
       "       [ 0.63821357,  0.88173421,  0.88468037,  0.88893963,  0.89124989,\n",
       "         0.90299125,  0.92625048,  0.93074588,  0.93920224,  0.88849843]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.v[:,:,0] #usable=False, the first three rows are 0 because these states won't occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.27704056,  0.09063411,  0.10520369,  0.13140702,  0.15802321,\n",
       "         0.19327823,  0.16632398,  0.09909412, -0.00682624, -0.12842951],\n",
       "       [-0.29395755,  0.04603198,  0.07181074,  0.10356778,  0.13115259,\n",
       "         0.16218827,  0.12270608,  0.05134397, -0.03980278, -0.16022003],\n",
       "       [-0.32731873,  0.02383484,  0.04820281,  0.08285757,  0.11406321,\n",
       "         0.13705981,  0.07860004,  0.01477721, -0.07526578, -0.19336646],\n",
       "       [-0.35298568, -0.00380607,  0.03413768,  0.0602127 ,  0.09173114,\n",
       "         0.11993422,  0.03438388, -0.02689627, -0.11595637, -0.22752306],\n",
       "       [-0.37949377, -0.02368671,  0.0106243 ,  0.04162766,  0.07261679,\n",
       "         0.09533093, -0.00359036, -0.06823445, -0.14597058, -0.25907776],\n",
       "       [-0.39194108,  0.00302596,  0.0282673 ,  0.05856883,  0.08650512,\n",
       "         0.12645328,  0.0549674 , -0.0737829 , -0.1512881 , -0.25089147],\n",
       "       [-0.33410945,  0.12321005,  0.1518942 ,  0.17557773,  0.20239666,\n",
       "         0.28466592,  0.40164557,  0.10318581, -0.10267637, -0.20019488],\n",
       "       [-0.11381807,  0.38494555,  0.40433101,  0.42022319,  0.44049703,\n",
       "         0.49307884,  0.61669566,  0.59524763,  0.28842836, -0.02052151],\n",
       "       [ 0.14483918,  0.63903544,  0.64992713,  0.66086626,  0.66864106,\n",
       "         0.70542957,  0.76995114,  0.79225035,  0.76001195,  0.43545206],\n",
       "       [ 0.63901146,  0.88205322,  0.8852785 ,  0.88885029,  0.89190812,\n",
       "         0.90317105,  0.92612428,  0.93079141,  0.93939303,  0.88857536]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.v[:,:,1] #usable=True, the first 11 rows are 0 because these states won't occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,  74529, 571753],\n",
       "       [     0,      0,      0]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i,j,k = 20,2,0\n",
    "optimal_policy.stats[i,j,k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: What's the house edge for this optimal policy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use MC to compute prob(i,j,k)\n",
    "prob = np.zeros((21,10,2))\n",
    "np.random.seed(4)\n",
    "max_iter = int(1e5) #reduce max_iter to 1e6 generates correct result except for the case player=12, dealer=4, use_able=0 \n",
    "for _ in range(max_iter):\n",
    "    dealer_init_cards = np.random.choice(learner.game.cards,size=2,replace=True)\n",
    "    player_init_cards = np.random.choice(learner.game.cards,size=2, replace=True)\n",
    "    i,j,k = convert_cards_to_state(player_init_cards, dealer_init_cards[0])\n",
    "    prob[i,j,k] = prob[i,j,k] + 1.0 \n",
    "prob = prob/max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999989"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " House edge:  -0.0463805599386\n"
     ]
    }
   ],
   "source": [
    "#E[Reture]= sum_{i,j,k} Prob[i,j,k] * E[Reture|i,j,k], E[Return|i,j,k]= optimal_policy.v\n",
    "print \"House edge: \", np.sum(prob*optimal_policy.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
